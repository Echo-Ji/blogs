<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>注意力机制 | Echo Ji</title>
    <meta name="description" content="
如果从源头来讲，注意力机制（Attention）最先是应用在 NLP 领域的机器翻译任务上的。
之前的机器翻译任务通常是 Seq2Seq 模型（Encoder-Decoder 结构）来解决的，Encoder 负责学习句子的表征，
将其总结为一个定长的向量（hidden vector），然后输入给 Decoder 来解码翻译，但注意只有第一个 Decoder 接收 hidden stat ...">
    <link rel="icon" href="/blogs/favicon.ico">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
    
    <link rel="preload" href="/blogs/assets/css/0.styles.606e9f23.css" as="style"><link rel="preload" href="/blogs/assets/js/app.67f6c0ad.js" as="script"><link rel="preload" href="/blogs/assets/js/4.7c2c95e9.js" as="script"><link rel="preload" href="/blogs/assets/js/5.8b86248a.js" as="script"><link rel="preload" href="/blogs/assets/js/42.792f151c.js" as="script"><link rel="prefetch" href="/blogs/assets/js/1.e76662eb.js"><link rel="prefetch" href="/blogs/assets/js/10.c6aeaa4b.js"><link rel="prefetch" href="/blogs/assets/js/11.82df316d.js"><link rel="prefetch" href="/blogs/assets/js/12.eab284d2.js"><link rel="prefetch" href="/blogs/assets/js/13.cd8e489a.js"><link rel="prefetch" href="/blogs/assets/js/14.a3f84c0c.js"><link rel="prefetch" href="/blogs/assets/js/15.9bdbb339.js"><link rel="prefetch" href="/blogs/assets/js/16.f4317575.js"><link rel="prefetch" href="/blogs/assets/js/17.fa69ee94.js"><link rel="prefetch" href="/blogs/assets/js/18.d453431b.js"><link rel="prefetch" href="/blogs/assets/js/19.064e46d1.js"><link rel="prefetch" href="/blogs/assets/js/20.c1c3c0e8.js"><link rel="prefetch" href="/blogs/assets/js/21.04422038.js"><link rel="prefetch" href="/blogs/assets/js/22.513643ab.js"><link rel="prefetch" href="/blogs/assets/js/23.c151a0a2.js"><link rel="prefetch" href="/blogs/assets/js/24.2308ffa1.js"><link rel="prefetch" href="/blogs/assets/js/25.ead0a5d4.js"><link rel="prefetch" href="/blogs/assets/js/26.0ef4d587.js"><link rel="prefetch" href="/blogs/assets/js/27.f8f8a9f8.js"><link rel="prefetch" href="/blogs/assets/js/28.f37568cf.js"><link rel="prefetch" href="/blogs/assets/js/29.cc7c7c41.js"><link rel="prefetch" href="/blogs/assets/js/30.61a41790.js"><link rel="prefetch" href="/blogs/assets/js/31.4f93a2f0.js"><link rel="prefetch" href="/blogs/assets/js/32.3007c773.js"><link rel="prefetch" href="/blogs/assets/js/33.015d00b6.js"><link rel="prefetch" href="/blogs/assets/js/34.5b8ce30c.js"><link rel="prefetch" href="/blogs/assets/js/35.2c376d79.js"><link rel="prefetch" href="/blogs/assets/js/36.68b803bc.js"><link rel="prefetch" href="/blogs/assets/js/37.61275c14.js"><link rel="prefetch" href="/blogs/assets/js/38.e3e5c80e.js"><link rel="prefetch" href="/blogs/assets/js/39.d17125e0.js"><link rel="prefetch" href="/blogs/assets/js/40.00614145.js"><link rel="prefetch" href="/blogs/assets/js/41.dc745971.js"><link rel="prefetch" href="/blogs/assets/js/43.e40bff08.js"><link rel="prefetch" href="/blogs/assets/js/6.0217efa2.js"><link rel="prefetch" href="/blogs/assets/js/7.c3500f45.js"><link rel="prefetch" href="/blogs/assets/js/8.dc8dfe73.js"><link rel="prefetch" href="/blogs/assets/js/9.119b9b2f.js"><link rel="prefetch" href="/blogs/assets/js/vuejs-paginate.e1db4a3b.js">
    <link rel="stylesheet" href="/blogs/assets/css/0.styles.606e9f23.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div id="vuepress-theme-blog__global-layout"><section id="header-wrapper"><header id="header"><div class="header-wrapper"><div class="title"><a href="/blogs/" class="nav-link home-link">Echo Ji </a></div> <div class="header-right-wrap"><ul class="nav"><li class="nav-item"><a href="/blogs/" class="nav-link">Blog</a></li><li class="nav-item"><a href="/blogs/tag/" class="nav-link">Tags</a></li></ul> <div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></div></header></section> <div id="mobile-header"><div class="mobile-header-bar"><div class="mobile-header-title"><a href="/blogs/" class="nav-link mobile-home-link">Echo Ji </a> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></div> <div class="mobile-menu-wrapper"><hr class="menu-divider"> <ul class="mobile-nav"><li class="mobile-nav-item"><a href="/blogs/" class="nav-link">Blog</a></li><li class="mobile-nav-item"><a href="/blogs/tag/" class="nav-link">Tags</a></li> <li class="mobile-nav-item"><!----></li></ul></div></div></div> <div class="content-wrapper"><div id="vuepress-theme-blog__post-layout"><div class="vuepress-blog-theme-content"><h1 class="post-title">注意力机制</h1> <div class="post-meta"><div class="post-meta-author"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-navigation"><polygon points="3 11 22 2 13 21 11 13 3 11"></polygon></svg> Echo
    <span>   in Beijing</span></div> <div class="post-meta-date"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg> 2020-11-20
  </div> <ul class="post-meta-tags"><li class="post-tag" data-v-d832e844><a href="/blogs/tag/注意力机制" data-v-d832e844> 注意力机制 </a></li><li class="post-tag" data-v-d832e844><a href="/blogs/tag/Attention" data-v-d832e844> Attention </a></li><li class="post-tag" data-v-d832e844><a href="/blogs/tag/序列模型" data-v-d832e844> 序列模型 </a></li><li class="post-tag" data-v-d832e844><a href="/blogs/tag/Transformer" data-v-d832e844> Transformer </a></li></ul></div> <div class="content__default"><h2 id="_1-从-seq2seq-说起"><a href="#_1-从-seq2seq-说起" class="header-anchor">#</a> 1. 从 Seq2Seq 说起</h2> <p>如果从源头来讲，注意力机制（Attention）最先是应用在 NLP 领域的机器翻译任务上的。
之前的机器翻译任务通常是 Seq2Seq 模型（Encoder-Decoder 结构）来解决的，Encoder 负责学习句子的表征，
将其总结为一个定长的向量（hidden vector），然后输入给 Decoder 来解码翻译，但注意只有第一个 Decoder 接收 hidden state，
之后的每个 Decoder 都将上一个 Decoder 的输出作为自己的输入。</p> <p>但是这个结构有个问题，对于较长的句子，很难保证最后的 hidden vector 能够保留所有有效信息，因此翻译的效果也会显著下降。</p> <p>为了解决由长序列到定长向量信息损失的问题，Attention 机制被引入了。Attention 机制的 motivation 来自于人类本身，
比如我们翻译句子的时候，往往只会关注翻译部分对应的上下文，同样 Attention 的思想也是给当前翻译词语的上下文更高的关注度。</p> <div style="text-align:center;"><iframe src="/blogs/2020-11-20-attention-ill.mp4" frameborder="0" allowfullscreen="allowfullscreen" style="margin:0 auto;"></iframe></div> <p>这里的关键操作就是，在 Deocder 进行翻译的时候不仅仅依赖于最后一个 Encoder 的输出，它会依赖于每一个 Encoder 的输出，根据这些输出计算一个 Attention 分布，
然后用 Attention 对所有 Encoder 的输出做加权，从而得到 Decoder 的输入，也就是说，使用 Attention 可以打破原有的使用单一定长向量的限制，
使得模型关注到被翻译词语的上下文（也许距离当前词语很远）。</p> <p>此外，通过可视化 Attention 矩阵，还可以更好地理解模型的工作机制。</p> <p>说到这里，读者也许会不禁想问，Attention 这么牛，他到底是怎么算的？</p> <h2 id="_2-细说-attention"><a href="#_2-细说-attention" class="header-anchor">#</a> 2. 细说 Attention</h2> <p>要说 Attention，先看一张图。</p> <div style="text-align:center;"><img src="/blogs/2020-11-20-attention-cal.png" alt="2020-11-20-attn-cal" style="margin:0 auto;"></div> <p>Attention 的计算通常有三个输入：<code>Q(uery), K(ey), V(alue)</code>，这三个输入通过图中的一波操作，即可得到加权后的结果。</p> <p>以上一节的 Seq2Seq 为例，<code>Query</code> 就是上一个 Decoder 的隐状态，<code>Key</code> 就是 Encoder 中的隐状态，<code>Value</code> 就是当前位置的 Encoder 的输出。</p> <div style="text-align:center;"><img src="/blogs/2020-11-20-normal-attention.jpg" alt="2020-11-20-norm-attn" style="margin:0 auto;"></div> <h2 id="_3-何为-self-attention"><a href="#_3-何为-self-attention" class="header-anchor">#</a> 3. 何为 Self-Attention</h2> <p>此外，还有一种特殊的 Attention，<code>Q, K, V</code> 均来自于同一个输入，通过不同的线性变换矩阵转化得到，Transformer 中就用的是这样的方式。</p> <p>关于 Self-Attention，<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">这篇文章<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>讲地非常棒，这里简单总结一下对于 Self-Attention 的求解过程：</p> <ul><li>计算 Attention score：计算当前 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">q_i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> 与每一个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">k_j</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03148em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> 的 Attention score（相似度），通常使用点积，即 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi></mtext><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>q</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\text{score}_{ij} = q_i \cdot k_j</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="text mord textstyle uncramped"><span class="mord mathrm">s</span><span class="mord mathrm">c</span><span class="mord mathrm">o</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">⋅</span><span class="mord"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03148em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>。</li> <li>归一化：为了梯度的稳定，对 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi></mtext></mrow><annotation encoding="application/x-tex">\text{score}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="text mord textstyle uncramped"><span class="mord mathrm">s</span><span class="mord mathrm">c</span><span class="mord mathrm">o</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span></span></span></span></span> 进行归一化，即除以 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.8572200000000001em;"></span><span class="strut bottom" style="height:1.04em;vertical-align:-0.18278em;"></span><span class="base textstyle uncramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.017220000000000013em;"><span class="style-wrap reset-textstyle textstyle uncramped">√</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.77722em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span>。</li> <li>Softmax 激活：将归一化后的结果用 Softmax 激活成 Attention 分布。</li> <li>加权求和：在得到 Attention 分布之后对每一个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">v</span></span></span></span> 进行加权，得到最终的表征向量。</li></ul> <p>向量化之后就可以表示为：</p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>n</mi><mo>(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo>)</mo><mo>=</mo><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mtext><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><mrow><msqrt><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></msqrt></mrow></mfrac><mo>)</mo><mi>V</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">Attn(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V.
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:1.5183309999999999em;"></span><span class="strut bottom" style="height:2.448331em;vertical-align:-0.9300000000000002em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">A</span><span class="mord mathit">t</span><span class="mord mathit">t</span><span class="mord mathit">n</span><span class="mopen">(</span><span class="mord mathit">Q</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mrel">=</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mopen">(</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.7472200000000002em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.017220000000000013em;"><span class="style-wrap reset-textstyle textstyle uncramped">√</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.77722em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">Q</span><span class="mord"><span class="mord mathit" style="margin-right:0.07153em;">K</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathit" style="margin-right:0.22222em;">V</span><span class="mord mathrm">.</span></span></span></span></span></p> <p>参考 <a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener noreferrer">1<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>、<a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener noreferrer">2<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>。</p></div> <!----> <hr> <!----></div> <div class="sticker vuepress-toc"><div class="vuepress-toc-item vuepress-toc-h2 active"><a href="#_1-从-seq2seq-说起" title="1. 从 Seq2Seq 说起">1. 从 Seq2Seq 说起</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#_2-细说-attention" title="2. 细说 Attention">2. 细说 Attention</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#_3-何为-self-attention" title="3. 何为 Self-Attention">3. 何为 Self-Attention</a></div></div></div></div> <footer class="footer" data-v-582f9766><div class="footer-left-wrap" data-v-582f9766><ul class="contact" data-v-582f9766><li class="contact-item" data-v-582f9766><a href="https://github.com/Echo-Ji" target="_blank" rel="noopener noreferrer" class="nav-link external" data-v-582f9766><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github" data-v-582f9766><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22" data-v-582f9766></path></svg>
          
        </a></li><li class="contact-item" data-v-582f9766><a href="/blogs/2020/11/20/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/bh1506jjh@gmail.com.html" class="nav-link" data-v-582f9766><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-mail" data-v-582f9766><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z" data-v-582f9766></path><polyline points="22,6 12,13 2,6" data-v-582f9766></polyline></svg>
          
        </a></li></ul></div> <div class="footer-right-wrap" data-v-582f9766><ul class="copyright" data-v-582f9766><li class="copyright-item" data-v-582f9766><a href="https://github.com/Echo-Ji" target="_blank" rel="noopener noreferrer" class="nav-link external" data-v-582f9766>Echo Ji © 2020</a></li></ul></div></footer></div><div class="global-ui"></div></div>
    <script src="/blogs/assets/js/app.67f6c0ad.js" defer></script><script src="/blogs/assets/js/4.7c2c95e9.js" defer></script><script src="/blogs/assets/js/5.8b86248a.js" defer></script><script src="/blogs/assets/js/42.792f151c.js" defer></script>
  </body>
</html>
