(window.webpackJsonp=window.webpackJsonp||[]).push([[32],{302:function(t,e,a){"use strict";a.r(e);var n=a(5),i=Object(n.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"bagging-与-boosting"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bagging-与-boosting"}},[t._v("#")]),t._v(" Bagging 与 Boosting")]),t._v(" "),a("h3",{attrs:{id:"bagging-方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bagging-方法"}},[t._v("#")]),t._v(" Bagging 方法")]),t._v(" "),a("ul",[a("li",[t._v("Bootstrap 采样生成相互独立的 k 个训练集")]),t._v(" "),a("li",[t._v("用这 k 个训练集训练 k 个模型")]),t._v(" "),a("li",[t._v("用这 k 个模型投票或平均的结果作为模型的输出")])]),t._v(" "),a("p",[t._v("例如 Bagging + 决策树 = 随机森林。")]),t._v(" "),a("p",[t._v("随机森林中要注意两个随机，一是为单棵决策树随机有放回地从大小为 N 的训练集中抽取 N 个样本，二是在决策树的每个节点进行分裂时随机从 M 个属性中抽取 m 个属性（m << M），然后从中选择一个最好的进行分裂。")]),t._v(" "),a("p",[t._v("注意，对于第二点而言，如果某节点选择的分裂属性与父节点相同，则该节点停止分裂，变成子节点。")]),t._v(" "),a("h3",{attrs:{id:"boosting-方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#boosting-方法"}},[t._v("#")]),t._v(" Boosting 方法")]),t._v(" "),a("ul",[a("li",[t._v("每一轮训练都改变数据的概率分布，提升误分类样本的权重，降低正确分类样本的权重")]),t._v(" "),a("li",[t._v("给错误率小的基础模型更大的权重，同时减小错误率高的模型权重")]),t._v(" "),a("li",[t._v("对基础模型进行加权组合，给出最后的结果")])]),t._v(" "),a("p",[t._v("AdaBoost，GBDT，XGBoost 都是这类方法的例子。")]),t._v(" "),a("h3",{attrs:{id:"对比"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#对比"}},[t._v("#")]),t._v(" 对比")]),t._v(" "),a("p",[t._v("两种方法对比如下：")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}}),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("Bagging")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("Boosting")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("数据选择")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("采用 Bootstrap 方法有放回地产生相互独立的训练集")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("训练集不变，但每个样本的权重会根据上一轮的分类结果进行调整")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("样本权重")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("每个样本权重一致")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("根据错误率调整权重，错误率大的下一轮学习中的权重高，以便学习器注意到该样本")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("预测函数")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("所有模型权重一致")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("每个弱分类器都有相应权重，分类效果好的权重高，反之则低")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("并行计算")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("每个弱分类器的训练互不干扰，可以并行")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("每一轮的计算都依赖于上一轮模型的分类结果，不能并行 🙅")])])])]),t._v(" "),a("p",[t._v("通常情况下，用 Bagging 方法得到的结果方差更小，而 Boosting 方法得到的结果偏差更小。为了考虑偏差和方差的权衡，就要尽量降低 Bagging 模型的偏差（构建相对复杂的模型），降低 Boosting 模型的方差（简化弱分类器）。")]),t._v(" "),a("p",[t._v("参考 "),a("a",{attrs:{href:"https://medium.com/@pkqiang49/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3-bagging-boosting-%E4%BB%A5%E5%8F%8A%E4%BB%96%E4%BB%AC%E7%9A%84-4-%E7%82%B9%E5%8C%BA%E5%88%AB-6e3c72df05b8",target:"_blank",rel:"noopener noreferrer"}},[t._v("1"),a("OutboundLink")],1),t._v("、"),a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/27689464",target:"_blank",rel:"noopener noreferrer"}},[t._v("2"),a("OutboundLink")],1),t._v("。")]),t._v(" "),a("h2",{attrs:{id:"blending-与-stacking"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#blending-与-stacking"}},[t._v("#")]),t._v(" Blending 与 Stacking")]),t._v(" "),a("p",[t._v("Blending 与 Stacking 方法都是训练一个总模型来组合其他各个模型，首先需要训练多个模型，然后将各个模型的输出作为总模型的输入，用以得到最后的输出。")]),t._v(" "),a("p",[t._v("Stacking 和 Blending 最大的区别貌似体现在数据划分上，数据划分的不同导致训练过程也有所区别，对比如下。")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}}),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("Blending")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("Stacking")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("初级，训练")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("把训练数据划分为初级训练集和初级验证集，然后通过初级训练集训练 m 个初级模型，用初级验证集生成次级验证集，次级验证集的大小为（初级验证集的样本数，m）")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("K 折交叉验证划分训练数据集，初级模型每次生成 1/K 的次级特征，K 次生成一个完整的次级特征，分别用 m 个模型生成次级训练集，其大小为（训练集的样本数，m）")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("初级，测试")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("通过初级模型用初级测试集生成次级测试集")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("通过初级模型用初级测试集生成次级测试集，取 K 次的平均结果作为最后的次级测试集")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("次级，训练")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("次级验证集用于训练次级融合模型")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("次级训练集用于训练次级融合模型")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("次级，测试")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("次级测试集用于测试")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("次级测试集用于测试")])])])]),t._v(" "),a("p",[t._v("相较于 Stacking，Blending 的操作更为简单，但数据利用率也较低。")]),t._v(" "),a("p",[t._v("参考 "),a("a",{attrs:{href:"https://blog.csdn.net/weixin_43467711/article/details/100749340",target:"_blank",rel:"noopener noreferrer"}},[t._v("1"),a("OutboundLink")],1),t._v("、"),a("a",{attrs:{href:"https://blog.csdn.net/weixin_43467711/article/details/105258441",target:"_blank",rel:"noopener noreferrer"}},[t._v("2"),a("OutboundLink")],1),t._v("。")])])}),[],!1,null,null,null);e.default=i.exports}}]);